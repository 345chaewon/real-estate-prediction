import numpy as np
import pandas as pd
import datetime
import matplotlib
import matplotlib.pyplot as plt

# tensorflow 버전선택
%tensorflow_version 1.x    
import tensorflow as tf
tf.__version__

# 랜덤에 의해 똑같은 결과를 재현하도록 시드 설정
# 하이퍼파라미터를 튜닝하기 위한 용도(흔들리면 무엇때문에 좋아졌는지 알기 어려움)
tf.set_random_seed(777)

# Standardization
def data_standardization(x):
    x_np = np.asarray(x)
    return (x_np - x_np.mean()) / x_np.std()

# 너무 작거나 너무 큰 값이 학습을 방해하는 것을 방지하고자 정규화한다
# x가 양수라는 가정하에 최소값과 최대값을 이용하여 0~1사이의 값으로 변환
# Min-Max scaling
def min_max_scaling(x):
    x_np = np.asarray(x)
    return (x_np - x_np.min()) / (x_np.max() - x_np.min() + 1e-7) # 1e-7은 0으로 나누는 오류 예방차원

# 정규화된 값을 원래의 값으로 되돌린다
# 정규화하기 이전의 org_x값과 되돌리고 싶은 x를 입력하면 역정규화된 값을 리턴한다
def reverse_min_max_scaling(org_x, x):
    org_x_np = np.asarray(org_x)
    x_np = np.asarray(x)
    return (x_np * (org_x_np.max() - org_x_np.min() + 1e-7)) + org_x_np.min()

from google.colab import files
files.upload()
!ls

filename = 'sheethome (1).xlsx'
xl = pd.ExcelFile(filename)
xl.sheet_names

apartments = input('아파트 후보군 : ').split(',')
month = int(input('예측 기간 : '))

# 추천을 위한 리스트 생성
recommand = []    # 아파트 이름
months = []     # recommand와 index를 맞춰서 가장 수익률이 높은 달을 따로 저장

# for문으로 입력받은 아파트를 하나하나 예측
for apartment in apartments: 
    print('\n\n\n< 아파트 이름 : ',apartment,'>\n\n\n')    # 현재 예측 중인 아파트 이름
    df = pd.read_excel(filename, sheet_name='pa')   # pa는 공통 변수 sheet
    apt = pd.read_excel(filename, sheet_name=apartment)    # 아파트별 sheet
    data = pd.merge(apt,df)    # df와 apt 표를 합침

    df2 = data[['매매가','전세가율','동행지수','CD','생산자물가지수','종합주가지수','소비자물가지수','지가변동률','아파트거래현황','주택건설 실적']]  # 여기에 해당하는 columns 들만 새로운 표로 만듦
    df2.columns=['price','index1','index2','index3','index4','index5','index6','index7','index8','index9']    # column 이름 새로 지정 


    # 하이퍼파라미터
    input_data_column_cnt = 10  # 입력데이터의 컬럼 개수(Variable 개수)
    output_data_column_cnt = month # 결과데이터의 컬럼 개수(원하는 예측 개월수)

    seq_length = 31            # 1개 시퀀스의 길이(시계열데이터 입력 개수)
    rnn_cell_hidden_dim = 20   # 각 셀의 (hidden)출력 크기
    forget_bias = 1.0          # 망각편향(기본값 1.0)
    num_stacked_layers = 1     # stacked LSTM layers 개수
    keep_prob = 1.0            # dropout할 때 keep할 비율

    epoch_num = 5000           # 에폭 횟수(학습용전체데이터를 몇 회 반복해서 학습할 것인가 입력)
    learning_rate = 0.05       # 학습률

    apt_info = df2.values[0:].astype(np.float)    # 위에서 만든 표를 전처리하기 편하게 array로 만듦

    # 데이터 전처리
    norm_apt_info = min_max_scaling(apt_info) # 데이터 정규화 처리

    # 행은 그대로 두고 열을 우측에 붙여 합친다
    x = norm_apt_info
    y = norm_apt_info[:, [0]] # 타켓은 아파트 가격이다

    dataX = [] # 입력으로 사용될 Sequence Data
    dataY = [] # 출력(타켓)으로 사용

    apt_price = []    # 추천을 위한 리스트,달마다 아파트값 넣기

    for i in range(0, len(y) - seq_length):
        _x = x[i : i+seq_length]
        _y = y[i + seq_length] # 다음 나타날 가격(정답)
        #if i is 0:
        #    print(_x, "->", _y) # 첫번째 행만 출력해 봄
        dataX.append(_x) # dataX 리스트에 추가
        dataY.append(_y) # dataY 리스트에 추가


    # 학습용/테스트용 데이터 생성 (80:10:10)
    # 전체 80%를 train 데이터로 사용
    train_size = int(len(dataY) * 0.8)
    # 10%를 validation 데이터로 사용
    valid_size = int(len(dataY)*0.1)
    # 나머지 10%를 test 데이터로 사용
    test_size = int(len(dataY)*0.1)

    # 데이터를 잘라 train 데이터 생성
    trainX = np.array(dataX[0:train_size])
    trainY = np.array(dataY[0:train_size])

    # 데이터를 잘라 valid 데이터 생성
    validX = np.array(dataX[train_size:train_size+valid_size])
    validY = np.array(dataY[train_size:train_size+valid_size])

    # 데이터를 잘라 test 데이터 생성
    testX = np.array(dataX[train_size+valid_size:len(dataX)])
    testY = np.array(dataY[train_size+valid_size:len(dataY)])
    
    # 모델 초기화 : for문으로 하나의 모델 껍데기를 여러번 이용해야하기때문에 초기화 과정이 필요하다. 
    tf.reset_default_graph()

    # 텐서플로우 플레이스홀더 생성
    # 입력 X, 출력 Y를 생성한다
    X = tf.placeholder(tf.float32, [None, seq_length, input_data_column_cnt])
    Y = tf.placeholder(tf.float32, [None, 1])

    # 검증용 측정지표를 산출하기 위한 targets, predictions를 생성한다
    targets = tf.placeholder(tf.float32, [None, 1])
    predictions = tf.placeholder(tf.float32, [None, output_data_column_cnt])

    # 모델(LSTM 네트워크) 생성
    def lstm_cell():
        # LSTM셀을 생성
        # num_units: 각 Cell 출력 크기
        # forget_bias:  to the biases of the forget gate
        #              (default: 1)  in order to reduce the scale of forgetting in the beginning of the training.
        # state_is_tuple: True ==> accepted and returned states are 2-tuples of the c_state and m_state.
        # state_is_tuple: False ==> they are concatenated along the column axis.
        cell = tf.contrib.rnn.BasicLSTMCell(num_units=rnn_cell_hidden_dim,
                                            forget_bias=forget_bias, state_is_tuple=True, activation=tf.nn.softsign)
        if keep_prob < 1.0:
            cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)
        return cell

    # num_stacked_layers개의 층으로 쌓인 Stacked RNNs 생성
    stackedRNNs = [lstm_cell() for _ in range(num_stacked_layers)]
    multi_cells = tf.contrib.rnn.MultiRNNCell(stackedRNNs, state_is_tuple=True) if num_stacked_layers > 1 else lstm_cell()

    # RNN Cell(여기서는 LSTM셀임)들을 연결
    hypothesis, _states = tf.nn.dynamic_rnn(multi_cells, X, dtype=tf.float32)
    print("hypothesis: ", hypothesis)

    # [:, -1]를 잘 살펴보자. LSTM RNN의 마지막 (hidden)출력만을 사용했다.
    # 과거 여러 가격를 이용해서 다음달의 가격 1개를 예측하기때문에 MANY-TO-ONE형태이다
    hypothesis = tf.contrib.layers.fully_connected(hypothesis[:, -1], output_data_column_cnt, activation_fn=tf.identity)

    # 손실함수로 평균제곱오차를 사용한다
    loss = tf.reduce_sum(tf.square(hypothesis - Y))
    # 최적화함수로 AdamOptimizer를 사용한다
    optimizer = tf.train.AdamOptimizer(learning_rate)
    # optimizer = tf.train.RMSPropOptimizer(learning_rate) # LSTM과 궁합 별로임
    train = optimizer.minimize(loss)

    # RMSE(Root Mean Square Error)
    # 제곱오차의 평균을 구하고 다시 제곱근을 구하면 평균 오차가 나온다
    rmse = tf.sqrt(tf.reduce_mean(tf.squared_difference(targets, predictions)))

    train_error_summary = []  # 학습용 데이터의 오류를 중간 중간 기록한다
    valid_error_summary = []  # validation 데이터의 오류를 중간 중간 기록한다
    test_error_summary = []  # 테스트용 데이터의 오류를 기록한다
    test_predict = ''  # 테스트용데이터로 예측한 결과

    sess = tf.Session()
    sess.run(tf.global_variables_initializer())

    # 학습한다
    start_time = datetime.datetime.now()  # 시작시간을 기록한다
    print('학습을 시작합니다...')
    for epoch in range(epoch_num):
        _, _loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})
        if ((epoch + 1) % 100 == 0) or (epoch == epoch_num - 1):  # 100번째마다 또는 마지막 epoch인 경우
            # 학습용데이터로 rmse오차를 구한다
            train_predict = sess.run(hypothesis, feed_dict={X: trainX})
            train_error = sess.run(rmse, feed_dict={targets: trainY, predictions: train_predict})
            train_error_summary.append(train_error)

            # validation 데이터로 rmse오차를 구한다
 lk           valid_predict = sess.run(hypothesis, feed_dict={X: validX})
            valid_error = sess.run(rmse, feed_dict={targets: validY, predictions: valid_predict})
            valid_error_summary.append(valid_error)

            # 현재 오류를 출력한다
            print("epoch: {}, train_error(A): {}, valid_error(B): {}, B-A: {}".format(epoch + 1, train_error, valid_error, valid_error - train_error))

    print('\ntrain_error : ', train_error, 'valid_error: ', valid_error,'\n')
    end_time = datetime.datetime.now()  # 종료시간을 기록한다
    elapsed_time = end_time - start_time  # 경과시간을 구한다
    print('모델 학습 경과 시간:', elapsed_time)
    print('1 epoch 모델 학습 시간:', elapsed_time / epoch_num,'\n')

    # 테스트 데이터 accuracy
    test_predict = sess.run(hypothesis, feed_dict = {X: testX})
    test_error = sess.run(rmse, feed_dict={targets: testY, predictions: test_predict})
    test_error_summary.append(test_error)
    accuracy = (1-test_error_summary[0])*100
    print('모델 정확도 : ',round(accuracy,3), '%\n\n')

    # 하이퍼파라미터 출력
    print('<하이퍼파라미터>\n')
    print('input_data_column_cnt:', input_data_column_cnt)
    print('output_data_column_cnt:', output_data_column_cnt)

    print('seq_length:', seq_length)
    print('rnn_cell_hidden_dim:', rnn_cell_hidden_dim)
    print('forget_bias:', forget_bias)
    print('num_stacked_layers:', num_stacked_layers)
    print('keep_prob:', keep_prob)

    print('epoch_num:', epoch_num)
    print('learning_rate:', learning_rate)

    print('train_error:', train_error_summary[-1])
    print('test_error:', valid_error_summary[-1])
    print('min_test_error:', np.min(valid_error_summary),'\n\n')


    # 결과 그래프 출력
    plt.figure(1)
    plt.plot(train_error_summary, 'gold')
    plt.plot(valid_error_summary, 'b')
    plt.xlabel('Epoch(x100)')
    plt.ylabel('Root Mean Square Error')
    plt.title('Error')
    plt.legend(labels=('train','valid'))
    print('\n')

    plt.figure(2)
    plt.plot(validY, 'r')
    plt.plot(valid_predict, 'b')
    plt.xlabel('Time Period')
    plt.ylabel('APT Price')
    plt.title('Prediction')
    plt.legend(labels=('real','prediction'))
    plt.show()
    print('\n')



    # sequence length만큼의 가장 최근 데이터를 슬라이싱한다
    recent_data = np.array([x[len(x)-seq_length : ]])
    # print("recent_data.shape:", recent_data.shape)
    # print("recent_data:", recent_data)

    # 아파트 가격 예측
    test_predict = sess.run(hypothesis, feed_dict={X: recent_data})

    # print("test_predict", test_predict[0])    # test data의 정규화된 예측값
    test_predict = reverse_min_max_scaling(apt_info,test_predict) # test data 의 예측값 역정규화

    validY_price = reverse_min_max_scaling(apt_info,validY)    # valid data의 Y값 역정규화
    valid_predict_price = reverse_min_max_scaling(apt_info, valid_predict)    # valid data의 prediction값 역정규화


    # 실제금액으로 역정규화한 그래프

    plt.figure(3)
    plt.plot(validY_price,'r')
    plt.plot(valid_predict_price, 'b')
    plt.xlabel("Time Period")
    plt.ylabel("APT price")
    plt.title("Prediction in real money")
    plt.legend(labels=('real','prediction'))
    plt.show()


    # print(valid_predict_price)
    print("Next month's apt price", test_predict[0])    # 예측한 아파트 가격
    print("이번달 아파트 가격: ", apt_info[-1][0])    # 이번달 아파트 가격
    apt_profit = 100*(test_predict[0]-apt_info[-1][0])/apt_info[-1][0]    # 수익률 계산
    print("앞으로의 수익률", apt_profit)
    m = np.where(max(apt_profit)==apt_profit)[0][0]+1
    print(apartment,'는', m,'달 후 수익률이',max(apt_profit), '% 로 가장 높습니다.')
    recommand.append(max(apt_profit))
    months.append(m)    




# 최종 추천
print('\n\n<아파트 별 최대 수익률>')
print(pd.Series(recommand,index=apartments))  # 아파트와 최대수익률 표
idx = recommand.index(max(recommand))     # 최대수익률이 가장 높은 것을 뽑아냄
print('\n추천 : ',apartments,'중에',apartments[idx],'의',months[idx],'달 후의 수익률이',max(recommand),'배로 가장 높습니다.')    # 추천 문구


df2.tail(10)

apt_info

reverse_min_max_scaling(apt_info,testY)




